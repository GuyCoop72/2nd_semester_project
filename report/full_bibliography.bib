@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:home/guy/Documents/2nd Semester Project/papers/RNN{\_}reg.pdf:pdf},
isbn = {078036404X},
issn = {0157244X},
number = {2013},
pages = {1--8},
pmid = {23259955},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Shi2017,
abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
archivePrefix = {arXiv},
arxivId = {1507.05717},
author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
doi = {10.1109/TPAMI.2016.2646371},
eprint = {1507.05717},
file = {:home/guy/Documents/2nd Semester Project/papers/RCNN{\_}for{\_}Text.pdf:pdf},
isbn = {2010110846},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Sequence recognition,convolutional neural network,long-short term memory,neural network,optical music recognition,scene text recognition},
number = {11},
pages = {2298--2304},
title = {{An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition}},
volume = {39},
year = {2017}
}
@article{Lipton2015,
abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
archivePrefix = {arXiv},
arxivId = {1506.00019},
author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
doi = {10.1145/2647868.2654889},
eprint = {1506.00019},
file = {:home/guy/Documents/2nd Semester Project/papers/RNN{\_}explained.pdf:pdf},
isbn = {9781450330633},
issn = {9781450330633},
pages = {1--38},
pmid = {18267787},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
url = {http://arxiv.org/abs/1506.00019},
year = {2015}
}
@article{Mogren2016,
abstract = {Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.},
archivePrefix = {arXiv},
arxivId = {1611.09904},
author = {Mogren, Olof},
eprint = {1611.09904},
file = {:home/guy/Documents/2nd Semester Project/papers/C{\_}RNN{\_}GAN.pdf:pdf},
number = {Nips},
title = {{C-RNN-GAN: Continuous recurrent neural networks with adversarial training}},
url = {http://arxiv.org/abs/1611.09904},
year = {2016}
}
@article{Dong2017,
abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/ .},
archivePrefix = {arXiv},
arxivId = {1709.06298},
author = {Dong, Hao-Wen and Hsiao, Wen-Yi and Yang, Li-Chia and Yang, Yi-Hsuan},
eprint = {1709.06298},
file = {:home/guy/Documents/2nd Semester Project/papers/MUSE{\_}GAN.pdf:pdf},
title = {{MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment}},
url = {http://arxiv.org/abs/1709.06298},
year = {2017}
}
@article{Yang2017,
abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
archivePrefix = {arXiv},
arxivId = {1703.10847},
author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
eprint = {1703.10847},
file = {:home/guy/Documents/2nd Semester Project/papers/MIDI{\_}net.pdf:pdf},
title = {{MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation}},
url = {http://arxiv.org/abs/1703.10847},
year = {2017}
}
@article{Elgammal2017,
abstract = {We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales.},
archivePrefix = {arXiv},
arxivId = {1706.07068},
author = {Elgammal, Ahmed and Liu, Bingchen and Elhoseiny, Mohamed and Mazzone, Marian},
eprint = {1706.07068},
file = {:home/guy/Documents/2nd Semester Project/papers/art{\_}style{\_}norms.pdf:pdf},
number = {Iccc},
pages = {1--22},
title = {{CAN: Creative Adversarial Networks, Generating "Art" by Learning About Styles and Deviating from Style Norms}},
url = {http://arxiv.org/abs/1706.07068},
year = {2017}
}
@article{Goodfellow2014,
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
file = {:home/guy/Documents/2nd Semester Project/papers/GAN{\_}Tutorial.pdf:pdf},
journal = {Nips'2014},
number = {Arxiv report 1406.2661},
title = {{Generative Adversarial Networks}},
year = {2014}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:home/guy/Documents/2nd Semester Project/papers/DCGAN.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}

@book{Wundt1874,
author = {Wundt, Willhelm Max},
file = {:home/guy/Documents/2nd Semester Project/papers/grundzgederphy00wund.pdf:pdf},
title = {{Grundz{\"{u}}ge de physiologischen Psychologie}},
year = {1874}
}